{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "print(\"torch.version=\", torch.__version__)\n",
    "print(\"device=\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkpoints import  models, word_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_width=5):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search.\n",
    "    \n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param image_path: path to image\n",
    "    :param word_map: word map\n",
    "    :param beam_size: number of sequences to consider at each decode-step\n",
    "    :return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_map)\n",
    "    word_map_start = word_map['<start>']\n",
    "    word_map_end = word_map['<end>']\n",
    "\n",
    "    # Read image and process\n",
    "    image = read_image_and_resize(image_path)\n",
    "\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "    \n",
    "    # Decode\n",
    "    seq, alphas = decode_one(decoder, encoder_out, encoder_dim, enc_image_size, word_map_start, word_map_end, beam_width)\n",
    "    return seq, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "    for x in zip(models, word_maps):\n",
    "        model, word_map = x\n",
    "        print(\"Model: \",model)\n",
    "        # Load models\n",
    "        checkpoint = torch.load(model, map_location=str(device))\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder = decoder.to(device)\n",
    "        decoder.eval()\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "\n",
    "        # Load word map (word2ix)\n",
    "        with open(word_map, 'r') as j:\n",
    "            word_map = json.load(j)\n",
    "        rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "        # Encode, decode with attention and beam search\n",
    "        for beam_width in [5]:\n",
    "            print(\"Beam Size = \",beam_width)\n",
    "            seq, alphas = caption_image_beam_search(encoder, decoder, img, word_map, beam_width)\n",
    "            alphas = torch.FloatTensor(alphas.to(\"cpu\"))\n",
    "\n",
    "            decoded_seq = []\n",
    "            for item in seq:\n",
    "                decoded_seq.append(rev_word_map[item])\n",
    "            print(decoded_seq)\n",
    "\n",
    "        # Visualize caption and attention of best sequence\n",
    "            visualize_att(img, seq, alphas, rev_word_map, smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from IPython.display import Image as DisplayImage\n",
    "#directory = '../data/flickr8k_images/'\n",
    "directory = '../data/images/test/'\n",
    "#directory = '../testimage/'\n",
    "for filename in random.sample(os.listdir(directory), 1):\n",
    "    if filename.endswith(\"jpg\") == False: continue\n",
    "    #img = \"../data/flickr8k_images/\"+str(line)[0:-2]+\".jpg\"\n",
    "    print(filename)\n",
    "    img=directory+filename\n",
    "    display(DisplayImage(img, width=150, height=150))\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
