{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname='../library/heiti.ttf', size=12)\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "#from scipy.misc import imresize\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "print(\"torch.version=\", torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_name1 = \"flickr30kzh_5_cap_per_img_5_min_word_freq_seg_based\"\n",
    "main_name2 = \"flickr30kzh_5_cap_per_img_5_min_word_freq_char_based\"\n",
    "main_name3 = \"flickr8kzh_5_cap_per_img_5_min_word_freq_seg_based\"\n",
    "main_name4 = \"flickr8kzh_5_cap_per_img_5_min_word_freq_char_based\"\n",
    "\n",
    "VGG19 = \"VGG19_\"\n",
    "#ft = \"_fine_tune\"\n",
    "ft = ''\n",
    "model1 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name1 + ft + \".pth.tar\"\n",
    "model2 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name2 + ft + \".pth.tar\"\n",
    "model3 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name3 + ft + \".pth.tar\"\n",
    "model4 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name4 + ft + \".pth.tar\"\n",
    "\n",
    "word_map1 =\"../prepared_data/WORDMAP_\" + main_name1 + \".json\"\n",
    "word_map2 =\"../prepared_data/WORDMAP_\" + main_name2 + \".json\"\n",
    "word_map3 =\"../prepared_data/WORDMAP_\" + main_name3 + \".json\"\n",
    "word_map4 =\"../prepared_data/WORDMAP_\" + main_name4 + \".json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_and_resize(image_path):\n",
    "    # Read image and process\n",
    "    img = imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    #img = imresize(img, (256, 256))\n",
    "    img = np.array(Image.fromarray(img).resize((256,256), Image.BICUBIC))\n",
    "    \n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image_greedy_search(encoder, decoder, image_path, word_map):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search.\n",
    "    \n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param image_path: path to image\n",
    "    :param word_map: word map\n",
    "    :param beam_size: number of sequences to consider at each decode-step\n",
    "    :return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = len(word_map)\n",
    "    word_map_start = word_map['<start>']\n",
    "    word_map_end = word_map['<end>']\n",
    "    image = read_image_and_resize(image_path)\n",
    "\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # Generate\n",
    "    seq, alphas = decode(decoder, encoder_out, encoder_dim, enc_image_size, word_map_start, word_map_end)\n",
    "    return seq, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n",
    "    \"\"\"\n",
    "    Visualizes caption with weights at every word.\n",
    "\n",
    "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
    "\n",
    "    :param image_path: path to image that has been captioned\n",
    "    :param seq: caption\n",
    "    :param alphas: weights\n",
    "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
    "    :param smooth: smooth weights?\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n",
    "\n",
    "    words = [rev_word_map[ind] for ind in seq]\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    for t in range(len(words)):\n",
    "        if t > 50:\n",
    "            break\n",
    "        \n",
    "        plt.subplot(np.ceil(len(words) / 10.), 10, t + 1)\n",
    "\n",
    "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12, fontproperties=font)\n",
    "        plt.imshow(image)\n",
    "        current_alpha = alphas[t, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.detach().numpy(), upscale=24, sigma=8)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.detach().numpy(), [14 * 24, 14 * 24])\n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.8)\n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(decoder, encoder_out, encoder_dim, enc_image_size, word_map_start, word_map_end):\n",
    "    \"\"\"Generate one sample\"\"\"\n",
    "    pre_word = torch.Tensor([word_map_start]).long().to(device)\n",
    "    top_scores = torch.zeros(1, 1).to(device)\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "    sampled_ids = [word_map_start]\n",
    "    alphas = torch.ones(1, enc_image_size, enc_image_size).to(device)\n",
    "    inputs = pre_word\n",
    "    while True:\n",
    "        embeddings = decoder.embedding(inputs)  # (1, embed_dim)\n",
    "        awe, alpha = decoder.attention(encoder_out, h)  # (1, encoder_dim), (1, num_pixels)\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size) # (1, enc_image_size, enc_image_size)\n",
    "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (1, encoder_dim)\n",
    "        awe = gate * awe\n",
    "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))\n",
    "        score = decoder.fc(h)  # (1, vocab_size)\n",
    "        preds = F.softmax(score, dim=1)\n",
    "        _, pred = preds.view(-1).topk(1)\n",
    "        sampled_ids.append(pred.item())\n",
    "        alphas=torch.cat((alphas, alpha), dim=0)\n",
    "        if pred == word_map_end: break\n",
    "        next_input = pred\n",
    "        inputs = torch.Tensor([next_input]).long().to(device)\n",
    "    return sampled_ids, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "    for x in [[model1, word_map1],[model2, word_map2],[model3, word_map3],[model4, word_map4]]:\n",
    "        model, word_map = x\n",
    "        # Load model\n",
    "        checkpoint = torch.load(model, map_location=str(device))\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder = decoder.to(device)\n",
    "        decoder.eval()\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "\n",
    "        # Load word map (word2ix)\n",
    "        with open(word_map, 'r') as j:\n",
    "            word_map = json.load(j)\n",
    "        rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "        # Encode, decode with attention and greedy search\n",
    "        seq, alphas = caption_image_greedy_search(encoder, decoder, img, word_map)\n",
    "        alphas = torch.FloatTensor(alphas.to(\"cpu\"))\n",
    "        decoded_seq = []\n",
    "        for item in seq:\n",
    "            decoded_seq.append(rev_word_map[item])\n",
    "        print(decoded_seq)\n",
    "        \n",
    "        # Visualize caption and attention of best sequence\n",
    "        visualize_att(img, seq, alphas, rev_word_map, smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from IPython.display import Image as DisplayImage\n",
    "directory = '../data/flickr30k_images/'\n",
    "#directory = '../data/images/test/'\n",
    "#directory = '../testimage/'\n",
    "for filename in random.sample(os.listdir(directory), 10):\n",
    "    if filename.endswith(\"jpg\") == False: continue\n",
    "    #img = \"../data/flickr8k_images/\"+str(line)[0:-2]+\".jpg\"\n",
    "    img=directory+filename\n",
    "    display(DisplayImage(img, width=150, height=150))\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
