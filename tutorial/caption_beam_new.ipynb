{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "print(\"torch.version=\", torch.__version__)\n",
    "print(\"device=\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_name1 = \"flickr30kzh_5_cap_per_img_5_min_word_freq_seg_based\"\n",
    "main_name2 = \"flickr30kzh_5_cap_per_img_5_min_word_freq_char_based\"\n",
    "main_name3 = \"flickr8kzh_5_cap_per_img_5_min_word_freq_seg_based\"\n",
    "main_name4 = \"flickr8kzh_5_cap_per_img_5_min_word_freq_char_based\"\n",
    "\n",
    "VGG19 = \"VGG19_\"\n",
    "#VGG19 = ''\n",
    "#ft = \"_fine_tune\"\n",
    "ft = ''\n",
    "model1 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name1 + ft + \".pth.tar\"\n",
    "model2 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name2 + ft + \".pth.tar\"\n",
    "model3 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name3 + ft + \".pth.tar\"\n",
    "model4 = \"../checkpoints/BEST_checkpoint_\" + VGG19 + main_name4 + ft + \".pth.tar\"\n",
    "\n",
    "word_map1 =\"../prepared_data/WORDMAP_\" + main_name1 + \".json\"\n",
    "word_map2 =\"../prepared_data/WORDMAP_\" + main_name2 + \".json\"\n",
    "word_map3 =\"../prepared_data/WORDMAP_\" + main_name3 + \".json\"\n",
    "word_map4 =\"../prepared_data/WORDMAP_\" + main_name4 + \".json\"\n",
    "\n",
    "beam_width = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_width=5):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search.\n",
    "    \n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param image_path: path to image\n",
    "    :param word_map: word map\n",
    "    :param beam_size: number of sequences to consider at each decode-step\n",
    "    :return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_map)\n",
    "    word_map_start = word_map['<start>']\n",
    "    word_map_end = word_map['<end>']\n",
    "\n",
    "    # Read image and process\n",
    "    image = read_image_and_resize(image_path)\n",
    "\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "    \n",
    "    # Decode\n",
    "    seq, alphas = decode_one(decoder, encoder_out, encoder_dim, enc_image_size, word_map_start, word_map_end, beam_width)\n",
    "    return seq, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "    for x in [[model1, word_map1],[model2, word_map2],[model3, word_map3],[model4, word_map4]]:\n",
    "        model, word_map = x\n",
    "        # Load models\n",
    "        checkpoint = torch.load(model, map_location=str(device))\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder = decoder.to(device)\n",
    "        decoder.eval()\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "\n",
    "        # Load word map (word2ix)\n",
    "        with open(word_map, 'r') as j:\n",
    "            word_map = json.load(j)\n",
    "        rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "        # Encode, decode with attention and beam search\n",
    "        seq, alphas = caption_image_beam_search(encoder, decoder, img, word_map, beam_width)\n",
    "        alphas = torch.FloatTensor(alphas.to(\"cpu\"))\n",
    "\n",
    "        decoded_seq = []\n",
    "        for item in seq:\n",
    "            decoded_seq.append(rev_word_map[item])\n",
    "        print(decoded_seq)\n",
    "\n",
    "        # Visualize caption and attention of best sequence\n",
    "        visualize_att(img, seq, alphas, rev_word_map, smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from IPython.display import Image as DisplayImage\n",
    "#directory = '../data/flickr30k_images/'\n",
    "#directory = '../data/images/test/'\n",
    "directory = '../testimage/'\n",
    "for filename in random.sample(os.listdir(directory), 10):\n",
    "    if filename.endswith(\"jpg\") == False: continue\n",
    "    #img = \"../data/flickr8k_images/\"+str(line)[0:-2]+\".jpg\"\n",
    "    img=directory+filename\n",
    "    display(DisplayImage(img, width=150, height=150))\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
